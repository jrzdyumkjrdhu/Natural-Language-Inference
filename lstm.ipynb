{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nos.makedirs(os.path.dirname('/kaggle/temp/'), exist_ok=True)\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pickle\nfrom tensorflow.keras.preprocessing.text import Tokenizer\n\ndef glove():\n    print(\"loading embeddings\")\n    with open('/kaggle/input/pickled-glove840b300d-for-10sec-loading/glove.840B.300d.pkl', 'rb') as fp:\n        embeddings = pickle.load(fp)\n    return embeddings\n\ndef matrix(corpus):\n    embeddings = glove()\n\n    print(\"training tokenizer\")\n    tokenizer = Tokenizer(num_words=50000)\n    tokenizer.fit_on_texts(corpus)\n\n    with open(\"/kaggle/temp/tokenizer.pickle\", \"wb\") as f:\n        pickle.dump(tokenizer, f)\n\n    index = tokenizer.word_index\n    emb_matrix = np.zeros((len(index) + 1, 300))\n\n    print(\"building embeddings matrix\")\n    for word, ind in index.items():\n        vector = embeddings.get(word)\n\n        if vector is not None:\n            emb_matrix[ind] = vector\n\n    return emb_matrix, tokenizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom tensorflow.keras.utils import to_categorical\nimport pickle\nfrom sklearn.model_selection import train_test_split\n\n\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n\ndef get_data(data: pd.DataFrame):\n    premises = data['premise'].to_numpy()\n    hypothesis = data['hypothesis'].to_numpy()\n    labels = np.zeros(premises.shape[0]) if 'label' not in data.columns else data['label']\n\n    all = [premises[i] + ' ' + hypothesis[i] for i in range(len(labels))]\n\n    return [premises, hypothesis, labels], all\n\n\ndef load_data():\n    traindata = pd.read_csv(\"/kaggle/input/contradictory-my-dear-watson/train.csv\")\n    testdata = pd.read_csv(\"/kaggle/input/contradictory-my-dear-watson/test.csv\")\n    # train, test = train_test_split(traindata, test_size=0.2)\n\n    return traindata, testdata\n\n\ndef process(data, tokenizer):\n    return pad_sequences(tokenizer.texts_to_sequences(data[0]), 50), \\\n        pad_sequences(tokenizer.texts_to_sequences(data[1]), 50), \\\n        to_categorical(data[2])\n\n\ndef preprocess_train(traindata):\n    data, text = get_data(traindata)\n\n    embeddings, tokenizer = matrix(text)\n\n    print(\"preprocessing train data\")\n    train = process(data, tokenizer)\n\n    return train, embeddings\n\n\ndef preprocess_test(testdata):\n    data, _ = get_data(testdata)\n\n    with open(\"/kaggle/temp/tokenizer.pickle\", \"rb\") as f:\n        tokenizer = pickle.load(f)\n\n    print(\"preprocessing test data\")\n    test = process(data, tokenizer)\n    return test\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow_addons as tfa\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:\n    strategy = tf.distribute.get_strategy() # for CPU and single GPU\n    print('Number of replicas:', strategy.num_replicas_in_sync)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom tensorflow.keras.layers import Input, Embedding, Dense, Dropout\nfrom tensorflow.keras.layers import Bidirectional, LSTM, concatenate\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import SGD, Adam\n\n\n\ndef BiLSTM(data):\n    with strategy.scope():\n        train, embeddings = preprocess_train(data)\n\n        embedding = Embedding(input_dim=embeddings.shape[0], output_dim=300, weights=[embeddings], input_length=50)\n        lstm = Bidirectional(LSTM(64))\n\n        premise_x = Input(shape=(100, ), dtype='int32')\n        hypothesis_x = Input(shape=(100, ), dtype='int32')\n\n        premise, hypothesis = embedding(premise_x), embedding(hypothesis_x)\n        premise, hypothesis = lstm(premise), lstm(hypothesis)\n\n        train_input = concatenate([premise, hypothesis])\n        train_input = Dropout(0.3)(train_input)\n\n        for _ in range(3):\n            train_input = Dense(600, activation='relu')(train_input)\n            train_input = Dropout(0.2)(train_input)\n\n        pred = Dense(3, activation='softmax')(train_input)\n\n\n        optimizer = SGD(0.01)\n        # optimizer = tfa.optimizers.SWA(optimizer)\n        model = Model(inputs=[premise_x, hypothesis_x], outputs=pred)\n        #optimizer = Adam()\n        model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n        model.summary()\n\n        print(\"training\")\n\n        callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=40, restore_best_weights=True)\n\n        history = model.fit(x=[train[0], train[1]],\n                            y=train[2],\n                            batch_size=256,\n                            epochs=600,\n                            validation_split=0.2,\n                            verbose=1,\n                            callbacks=[callback])\n\n        model.save(\"/kaggle/temp/BiLSTM.h5\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import load_model, Model\nimport pandas as pd\nimport numpy as np\n\n\ndef test_model(data):\n    # Preprocess the data\n    test = preprocess_test(data)\n    model: Model = load_model('/kaggle/temp/BiLSTM.h5')\n\n    # Evaluate the loaded model with test data\n    # loss, accuracy = model.evaluate(x=[test[0], test[1]], y=test[2], batch_size=16)\n    # print(\"Test Loss: {:.2f}, Test Accuracy: {:.2f}%\\n\".format(loss, (accuracy * 100)))\n\n    pred = np.argmax(model.predict([test[0], test[1]]), axis=1)\n    df: pd.DataFrame = pd.DataFrame({'prediction': pred}, index=data['id'])\n    df.to_csv(\"/kaggle/working/submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train, test = load_data()\n\nBiLSTM(train)\n\ntest_model(test)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}