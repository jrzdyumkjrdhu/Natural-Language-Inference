# Natural-Language-Inference

Kaggle submissions for the Natural Language Inference problem https://www.kaggle.com/c/contradictory-my-dear-watson.


* [lstm.ipynb](https://github.com/jrzdyumkjrdhu/Natural-Language-Inference/blob/main/lstm.ipynb): our baseline LSTM submission. Kaggle score 0.44812

* [bert_milestone_version.ipynb](https://github.com/jrzdyumkjrdhu/Natural-Language-Inference/blob/main/bert_milestone_version.ipynb): a basic working BERT model from the milestone presentation. Kaggle score 0.64581

* [roberta_version.ipynb](https://github.com/jrzdyumkjrdhu/Natural-Language-Inference/blob/main/roberta_version.ipynb): an improved submission that uses tf-xlm-roberta-large. Kaggle score 0.80500

* [roberta_xnli_version.ipynb](https://github.com/jrzdyumkjrdhu/Natural-Language-Inference/blob/main/roberta_xnli_version.ipynb): an attempt to use a model trained specifically for NLI. Kaggle score 92050

* [xnli_is_bad.ipynb](https://github.com/jrzdyumkjrdhu/Natural-Language-Inference/blob/main/xnli_is_bad.ipynb): the script comparing the xnli dataset to the Kaggle datasets

* [distilbert.ipynb](https://github.com/jrzdyumkjrdhu/Natural-Language-Inference/blob/main/distilbert.ipynb): all further experiments with distilbert
